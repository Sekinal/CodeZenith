import os
import openai
import memory as mm
import custom_command_line as cml

# Set the OpenAI API key from the environment variable
openai.api_key = os.getenv("OPENAI_API_KEY")

def gpt_memory_helper(prompt, model_name):
    """
    Generates a prompt for the memory agent to rescue relevant information.

    Args:
        prompt (str): The prompt to be used on the vector database.
        model_name (str): The name of the OpenAI model to use.
    
    Returns:
        str: The rescued relevant information for the introduced prompt.
    """

    # Create a chat completion request with the given prompt and system message
    response = openai.ChatCompletion.create(
        model=f"{model_name}",
        messages=[
            {"role": "system", "content": "You're an Artificial Intelligence assistant whose sole purpose is to create a prompt relevant to the information given by the user. This generated prompt's purpose is to extract relevant data from a vector database. Keep it as specific and simple as possible."},
            {"role": "user", "content": f"{prompt}"},
        ]
    )
    
    # Return the generated response from the chat completion request
    return response['choices'][0]['message']['content']

def gpt_answer(prompt, model_name):
    """
    Generates a response to a given prompt using a given OpenAI model.

    Args:
        prompt (str): The prompt to generate a response to.
        model_name (str): The name of the OpenAI model to use.

    Returns:
        str: The generated response to the prompt.
    """
    
    memory_prompt = gpt_memory_helper(prompt, model_name) # A prompt generated by a memory agent to obtain a better answer when
    # querying the vector index
    if not os.path.exists("memory") or not os.path.exists("memory/index.json"):
        memory_extraction = "This is the first message on the conversation. There's nothing stored yet."
    else:
        memory_extraction = mm.extract_from_memory(memory_prompt)
    # Create a chat completion request with the given prompt and system message
    response = openai.ChatCompletion.create(
        model=f"{model_name}",
        messages=[
            {"role": "system", "content": "You're an Artificial Intelligence coding assistant. You'll write the code requested by the user and/or give directions and ideas on how to build the requested software. If code is requested, always, under no exceptions, write the requested code as one block first and any example use cases after that (on separate code blocks). Include comments and docstrings. Make the code use the best of practices and extremely readable but concise."},
            {"role": "user", "content": f"Some previous information that could prove useful: {memory_extraction}"},
            {"role": "user", "content": f"{prompt}"},
        ]
    )
    
    cml.pretty_print_system(f'Memory prompt used: {memory_prompt}')
    cml.pretty_print_system(f'Memory extraction: {memory_extraction}')
    
    # Return the generated response from the chat completion request
    return response['choices'][0]['message']['content']